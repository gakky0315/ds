# Attention Is All You Need - 日本語翻訳

## 1. はじめに (Introduction)

リカレントニューラルネットワーク (RNN)、長短期記憶 (LSTM) [13]、およびゲート付きリカレントニューラルネットワーク (GRU) [7] は、言語モデリングや機械翻訳などの系列モデリングと変換問題において、現状最も優れた手法として確立されています [35, 2, 5]。これらのリカレントモデルは、入力と出力シーケンスのシンボル位置に沿って計算を行います。時間ステップに基づいてシーケンスを処理するため、計算はシーケンス長が長くなると並列化が困難になり、バッチ処理が制約されます。

一方、アテンションメカニズムは、入力や出力シーケンスに関係なく依存関係をモデル化できるため、さまざまなタスクにおいて強力なツールとなっています [2, 19]。従来はアテンション機構はリカレントネットワークと組み合わせて使用されてきましたが、本研究では、リカレント層を完全に排除し、アテンションメカニズムのみに基づく新しいモデル「Transformer」を提案します。このモデルは、従来のモデルと比較してより並列化が可能で、翻訳品質の向上とトレーニング時間の短縮を実現しました。

この論文では、Transformerモデルを説明し、自己アテンションの利点について詳述し、RNNや畳み込みネットワークと比較します。また、英語-ドイツ語と英語-フランス語の機械翻訳タスクにおいて、Transformerモデルが高い性能を示したことを報告します。

## 2. 背景 (Background)

系列計算の削減という目標は、Extended Neural GPU [16]、ByteNet [18]、およびConvS2S [9] などのモデルの基盤を形成しています。これらのモデルは、畳み込みニューラルネットワークを基本ブロックとして使用し、すべての入力および出力位置に対して隠れた表現を並列に計算します。しかし、これらのモデルでは、任意の入力または出力位置から別の位置まで信号を伝達するために必要な操作の数は、位置間の距離に比例して増加します。たとえば、ConvS2Sでは線形に、ByteNetでは対数的に増加します。このため、遠く離れた位置間の依存関係を学習することが難しくなります [12]。

一方、Transformerでは、この操作数を一定に抑えていますが、アテンションの重み付き位置の平均化による解像度の低下が生じます。この影響を、セクション3.2で説明する「Multi-Head Attention」によって軽減しています。

自己アテンション (Self-attention) または「内部アテンション (Intra-attention)」は、系列の異なる位置を関連付けて、系列の表現を計算するメカニズムです。自己アテンションは、読解、抽象的な要約、テキスト推論、タスクに依存しない文章表現の学習など、さまざまなタスクで成功を収めています [4, 27, 28, 22]。

エンドツーエンドメモリネットワークは、系列に沿った再帰ではなく、再帰的なアテンションメカニズムに基づいており、シンプルな言語の質問応答や言語モデリングタスクにおいても良好な結果を示しています [34]。

我々の知る限り、Transformerは、自己アテンションのみを使用して入力と出力の表現を計算し、系列に沿ったRNNや畳み込みを使用しない最初の変換モデルです。次のセクションでは、Transformerを詳細に説明し、自己アテンションを動機づけ、[17, 18]や[9]などのモデルと比較して、その利点を議論します。

## 3. モデルアーキテクチャ (Model Architecture)

最も競争力のあるニューラル系列変換モデルは、エンコーダ・デコーダ構造を持っています [5, 2, 35]。この構造では、エンコーダがシンボル表現の入力系列 (x1, ..., xn) を連続表現の系列 $z = (z_1, ..., z_n)$ にマッピングし、デコーダが $z$ を基にシンボルの出力系列 (y1, ..., ym) を1つずつ生成します。モデルは自己回帰的であり [10]、次のシンボルを生成する際には、これまでに生成されたシンボルを追加の入力として使用します。

Transformerはこのエンコーダ・デコーダ構造を採用し、両方で自己アテンションと位置ごとの完全接続層 (point-wise fully connected layers) を積み重ねた構造を使用しています。図1にエンコーダとデコーダのそれぞれのアーキテクチャが示されています。

## 3.1 エンコーダとデコーダのスタック (Encoder and Decoder Stacks)

**エンコーダ**  
エンコーダは、N=6の同一の層で構成されています。各層は2つのサブ層を持ちます。1つ目はマルチヘッド自己アテンションメカニズムで、2つ目は位置ごとの完全接続フィードフォワードネットワークです。我々は、各サブ層に対して残差接続 [11] を用い、その後に層正規化 [1] を行います。つまり、各サブ層の出力は、LayerNorm(x + Sublayer(x)) で表され、Sublayer(x) はサブ層自身が実装する関数です。これらの残差接続を可能にするため、モデル内のすべてのサブ層および埋め込み層は、出力次元 $d_{\text{model}} = 512$ を生成します。

**デコーダ**  
デコーダもまた、N=6の同一の層で構成されています。各エンコーダ層の2つのサブ層に加えて、デコーダはエンコーダスタックの出力に対してマルチヘッドアテンションを行う第3のサブ層を挿入します。エンコーダと同様に、我々は各サブ層の周りに残差接続を用い、その後層正規化を行います。さらに、デコーダスタックの自己アテンションサブ層を変更し、後続の位置に注意を払うことを防ぎます。このマスキングにより、予測は既知の出力にのみ依存するようになります。

## 3.2 アテンション (Attention)

アテンション関数は、クエリとキー-バリューペアのセットを出力にマッピングするものとして記述できます。ここで、クエリ、キー、バリュー、出力はすべてベクトルです。出力はバリューの重み付き和として計算され、重みはクエリと対応するキーとの互換性を示す関数によって決定されます。

### 3.2.1 スケール付きドットプロダクトアテンション (Scaled Dot-Product Attention)

我々が提案する特定のアテンションメカニズムは、「スケール付きドットプロダクトアテンション (Scaled Dot-Product Attention)」です（図2）。入力は、次元が $d_k$ のクエリとキー、次元が $d_v$ のバリューで構成されます。クエリとすべてのキーとのドット積を計算し、それを $\sqrt{d_k}$ で割った後、ソフトマックス関数を適用してバリューに対する重みを得ます。

実際には、クエリのセットに対して同時にアテンション関数を計算します。クエリは行列 $Q$ にまとめられ、キーとバリューはそれぞれ行列 $K$, $V$ にまとめられます。出力の行列は次のように計算されます。

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

最も一般的に使用されるアテンション関数は、加算アテンション (additive attention) [2] とドットプロダクトアテンション (multiplicative attention) です。ドットプロダクトアテンションは、スケーリングファクター $\frac{1}{\sqrt{d_k}}$ がある以外は、我々のアルゴリズムと同一です。加算アテンションは、1つの隠れ層を持つフィードフォワードネットワークを使用して互換性関数を計算します。理論上の複雑さは似ていますが、ドットプロダクトアテンションは実際にははるかに高速で、メモリ効率が高いです。

$d_k$ の値が小さい場合には、これらの2つのメカニズムはほぼ同じ性能を発揮しますが、大きい値では加算アテンションが優れた結果を出します [3]。我々は、大きな $d_k$ の場合、ドット積が大きくなりすぎ、ソフトマックス関数が極端に小さな勾配領域に入ってしまうことが原因だと考えています。この効果を緩和するため、ドット積を $\frac{1}{\sqrt{d_k}}$ でスケーリングしています。

### 3.2.2 マルチヘッドアテンション (Multi-Head Attention)

1つのアテンション関数を用いる代わりに、クエリ、キー、およびバリューをそれぞれ異なる線形変換により $h$ 回プロジェクトし、それぞれのプロジェクト後のクエリ、キー、バリューに対して並列にアテンション関数を適用します。これにより、異なる表現空間からの情報に同時に注意を払うことができるようになります。

マルチヘッドアテンションの計算式は次の通りです：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

各ヘッド $head_i$ は、以下のように計算されます：

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

ここで、行列 $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$, $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ は学習可能なパラメータ行列です。  
この作業では、 $h = 8$ の並列アテンション層を使用し、それぞれの $d_k = d_v = \frac{d_{\text{model}}}{h} = 64$ とします。各ヘッドの次元が減少しているため、全体の計算コストは、完全次元のシングルヘッドアテンションと同程度です。


### 3.2.3 我々のモデルにおけるアテンションの応用 (Applications of Attention in our Model)

Transformerは、3つの異なる方法でマルチヘッドアテンションを使用しています。

#### エンコーダ-デコーダアテンション
エンコーダ-デコーダアテンション層では、クエリは前のデコーダ層から来ており、メモリキーとバリューはエンコーダの出力から来ます。これにより、デコーダのすべての位置が入力系列のすべての位置に注意を向けることができます。このメカニズムは、一般的な系列-系列モデルにおけるエンコーダ-デコーダアテンション機構を模倣しています。

#### エンコーダの自己アテンション
エンコーダには自己アテンション層が含まれています。この層では、キー、バリュー、クエリがすべて同じ場所、つまりエンコーダの前の層の出力から供給されます。これにより、エンコーダ内の各位置が前の層のすべての位置に注意を払うことができます。

#### デコーダの自己アテンション
同様に、デコーダ内の自己アテンション層は、デコーダ内の各位置が、その位置を含むすべての位置に注意を払うことを可能にします。我々は、デコーダ内で左方向への情報フローを防ぐため、自己回帰性を維持する必要があります。これを実現するため、スケール付きドットプロダクトアテンションの内部で、ソフトマックスの入力における不正な接続に対応するすべての値をマスク (−∞ に設定) しています。

## 3.3 位置ごとのフィードフォワードネットワーク (Position-wise Feed-Forward Networks)

アテンションサブ層に加えて、我々のエンコーダおよびデコーダの各層には、全位置に対して別個に適用される完全接続フィードフォワードネットワークがあります。これは、ReLUアクティベーションを介した2つの線形変換で構成されています。

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

線形変換は異なる位置に対して同一ですが、層ごとに異なるパラメータが使用されます。別の表現として、これはカーネルサイズ1の2つの畳み込みとしても説明できます。入力および出力の次元は $d_{\text{model}} = 512$、内部層の次元は $d_{\text{ff}} = 2048$ です。

## 3.4 埋め込みとソフトマックス (Embeddings and Softmax)

他の系列変換モデルと同様に、我々は学習済みの埋め込みを使用して、入力トークンと出力トークンを次元 $d_{\text{model}}$ のベクトルに変換します。また、出力トークンを予測するために、通常の学習済み線形変換とソフトマックス関数を使用します。モデル内では、2つの埋め込み層とプリソフトマックス線形変換の間で同じ重み行列を共有し [30]、埋め込み層ではその重みを $d_{\text{model}}$ で乗算します。

## 3.5 位置エンコーディング (Positional Encoding)

我々のモデルには再帰や畳み込みがないため、系列の順序情報を利用するために、トークンの相対的または絶対的な位置に関する情報を注入する必要があります。この目的のため、エンコーダおよびデコーダのスタックの下部で、入力埋め込みに「位置エンコーディング (Positional Encoding)」を追加します。位置エンコーディングは埋め込みと同じ次元 $d_{\text{model}}$ を持ち、2つを加算できます。位置エンコーディングには、学習済みと固定の2つの選択肢があります [9]。

この研究では、異なる周波数の正弦波と余弦波を使用しています。

$$
PE(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{2i / d_{\text{model}}}}\right)
$$

$$
PE(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{2i / d_{\text{model}}}}\right)
$$

ここで、 $\text{pos}$ は位置、 $i$ は次元です。つまり、位置エンコーディングの各次元は正弦波に対応しており、その波長は2πから10000 * 2πの間で幾何学的に進行します。我々は、この関数が、モデルが相対位置に基づいて注意を向けることを容易に学習できると仮定して、この関数を選択しました。特定のオフセット $k$ に対して、 $PE_{\text{pos}+k}$ を $PE_{\text{pos}}$ の線形関数として表現できるためです。

我々は、代わりに学習済み位置埋め込みを使用する実験も行いましたが、2つのバージョンはほぼ同一の結果を示しました (表3の行(E) を参照)。トレーニング時に遭遇する系列長を超えるシーケンスに対しても外挿できる可能性があるため、正弦波バージョンを選択しました。

## 4. なぜ自己アテンションなのか (Why Self-Attention)

このセクションでは、自己アテンション層と、一般的に使用されるリカレント層および畳み込み層を比較します。これらの層は、変数長のシーケンス $(x_1, ..., x_n)$ から等しい長さのシーケンス $(z_1, ..., z_n)$ にマッピングするために使用されることが多く、 $x_i, z_i \in \mathbb{R}^d$ といった典型的なシーケンス変換エンコーダまたはデコーダ内の隠れ層の一部です。我々が自己アテンションを使用する理由を説明するため、3つの要件を考慮します。

1つ目は、**層ごとの計算複雑さ**です。もう1つは、**並列化できる計算量**です。これは、必要な逐次操作の最小数で測定されます。

3つ目は、ネットワーク内の**長距離依存関係のパス長**です。長距離依存関係を学習することは、シーケンス変換タスクにおいて重要な課題です。こうした依存関係を学習する能力に影響を与える要素の1つは、ネットワーク内で信号が前方および後方に伝わる経路の長さです。入力および出力の任意の位置間での経路が短いほど、長距離依存関係を学習しやすくなります [12]。したがって、異なる層タイプで構成されたネットワークにおいて、入力および出力の任意の位置間の最大経路長も比較します。

表1で示されているように、自己アテンション層は一定の逐次的な操作数で全ての位置を接続しますが、リカレント層は $O(n)$ の逐次操作が必要です。計算複雑さの観点では、シーケンス長 $n$ が表現の次元数 $d$ より小さい場合、自己アテンション層はリカレント層よりも高速です。これは、最先端の機械翻訳モデルで使用される文表現、例えば、word-piece [38] や byte-pair [31] の表現が該当します。非常に長いシーケンスを含むタスクの計算性能を向上させるためには、自己アテンションを入力シーケンス内の出力位置周辺のサイズ $r$ の近傍に限定することができます。これにより、最大経路長は $O(n/r)$ に増加します。このアプローチについては今後の研究でさらに検討する予定です。

カーネル幅 $k < n$ の単一の畳み込み層は、すべての入力および出力の位置を接続しません。これを実現するには、隣接カーネルの場合、 $O(n/k)$ の畳み込み層のスタックが必要です。あるいは、拡張畳み込みの場合は、 $O(\log_k(n))$ です [18]。これにより、ネットワーク内の任意の2つの位置間の最長経路が長くなります。畳み込み層は、リカレント層よりも一般的に計算コストが高く、ファクタ $k$ の差があります。ただし、セパラブル畳み込み [6] は、複雑さを大幅に削減し、 $O(k \cdot n \cdot d + n \cdot d^2)$ となります。それでも、 $k = n$ の場合、セパラブル畳み込みの複雑さは、自己アテンション層と位置ごとのフィードフォワード層の組み合わせと同じになります。このアプローチは、我々のモデルでも採用しています。

副次的なメリットとして、自己アテンションはより解釈しやすいモデルを生み出す可能性があります。我々は、モデルから得られたアテンション分布を調査し、付録に例を示して議論しています。個々のアテンションヘッドは明確に異なるタスクを学習するだけでなく、多くのヘッドは文の構文的および意味的な構造に関連した動作を示すようです。



## 5. トレーニング (Training)

このセクションでは、モデルのトレーニングに関する詳細を説明します。

### 5.1 トレーニングデータとバッチ処理 (Training Data and Batching)

我々は、約450万の文対を含む標準的なWMT 2014英語-ドイツ語データセットでトレーニングを行いました。文はバイトペアエンコーディング (BPE) [3] を使用してエンコードされ、約37,000トークンからなる共有ソースターゲット語彙が使用されました。英語-フランス語の場合、はるかに大きなWMT 2014英語-フランス語データセット (3,600万文) を使用し、32,000トークンからなる語彙に分割されました [38]。文対はおおよその文長でバッチ処理され、各トレーニングバッチには、約25,000のソーストークンと25,000のターゲットトークンが含まれていました。

### 5.2 ハードウェアとスケジュール (Hardware and Schedule)

我々は、8つのNVIDIA P100 GPUを備えた1台のマシンでモデルをトレーニングしました。我々のベースモデルは、論文全体で説明されているハイパーパラメータを使用しており、各トレーニングステップは約0.4秒かかりました。ベースモデルは合計100,000ステップ、すなわち12時間でトレーニングされました。我々のビッグモデル（表3の最下行に記載）は、ステップごとに1.0秒かかり、300,000ステップ (3.5日間) トレーニングされました。

### 5.3 オプティマイザ (Optimizer)

我々はAdamオプティマイザ [20] を使用し、ハイパーパラメータは $\beta_1 = 0.9$、 $\beta_2 = 0.98$、および $\epsilon = 10^{-9}$ としました。トレーニングの過程で、次の式に基づいて学習率を変化させました。

$$
\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})
$$

これは、トレーニングの最初の $warmup\_steps$ ステップでは学習率が線形に増加し、その後、ステップ数の逆平方根に比例して減少することを意味します。我々は $warmup\_steps = 4000$ を使用しました。

### 5.4 正則化 (Regularization)

トレーニング中、我々は3つの正則化手法を使用しました。

- **残差ドロップアウト (Residual Dropout)**: 各サブ層の出力にドロップアウト [33] を適用し、それをサブ層の入力に加算して正規化する前に適用します。さらに、エンコーダおよびデコーダスタック内の埋め込みと位置エンコーディングの和にもドロップアウトを適用しました。ベースモデルでは、ドロップアウト率 $P_{\text{drop}} = 0.1$ を使用しました。
  
- **ラベルスムージング (Label Smoothing)**: トレーニング中、我々はラベルスムージング [36] を使用し、その値は $\epsilon_{\text{ls}} = 0.1$ としました。これにより、モデルが確信を持たずに学習し、パープレキシティは悪化するものの、精度とBLEUスコアが向上しました。

## 6. 結果 (Results)

このセクションでは、我々のモデルが達成した結果について説明します。

### 6.1 機械翻訳 (Machine Translation)

WMT 2014英語-ドイツ語翻訳タスクにおいて、我々のビッグトランスフォーマーモデル（表2の「Transformer (big)」）は、これまでに報告された最良のモデル（アンサンブルも含む）を2.0以上のBLEUスコアで上回り、28.4の新しい最先端BLEUスコアを達成しました。このモデルの設定は表3の最下行に記載されています。トレーニングには8つのP100 GPUを使用して3.5日間かかりました。ベースモデルでさえ、すべての既存モデルやアンサンブルを凌駕し、競合するモデルと比較してわずかなトレーニングコストでこれを達成しました。

WMT 2014英語-フランス語翻訳タスクにおいても、我々のビッグモデルは41.0というBLEUスコアを達成し、これまでに報告されたすべてのシングルモデルを上回りました。これは、従来の最先端モデルのトレーニングコストの4分の1未満で達成されています。英語-フランス語タスクに使用したビッグモデルでは、ドロップアウト率 $P_{\text{drop}} = 0.1$ を使用しました（0.3ではありません）。

ベースモデルについては、10分ごとに書き出された最後の5つのチェックポイントを平均化して単一モデルを得ました。ビッグモデルについては、最後の20のチェックポイントを平均化しました。我々は、ビームサイズ4、長さペナルティ $\alpha = 0.6$ を使用したビームサーチを行いました。これらのハイパーパラメータは、開発セット上での実験に基づいて選択されました。推論時の最大出力長は、入力長に50を加えた値に設定しましたが、可能な場合には早期終了しました [38]。

表2は、我々の結果を要約し、我々の翻訳品質およびトレーニングコストを文献上の他のモデルアーキテクチャと比較しています。トレーニングに使用された浮動小数点演算数は、トレーニング時間、使用されたGPUの数、および各GPUの持続的な単精度浮動小数点能力の見積もりを掛け合わせて推定しました。

### 6.2 モデルのバリエーション (Model Variations)

Transformerの異なるコンポーネントの重要性を評価するために、我々はベースモデルに対してさまざまな変更を加え、英語-ドイツ語翻訳の性能を測定しました。表3に示すように、アテンションヘッドの数やアテンションキーとバリューの次元を変えて、パフォーマンスの変化を評価しました。また、サイズの違うモデル、ドロップアウト、ポジショナルエンコーディングの学習方法の影響を評価しました。

### 6.3 英語構成解析 (English Constituency Parsing)

我々のモデルが他のタスクに対しても一般化できるかどうかを評価するために、英語構成解析の実験を行いました。このタスクは、出力が入力よりもはるかに長く、厳しい構造的制約が課されるという特定の課題があります。また、小規模データセットでは、RNN系列-系列モデルは最先端の結果を達成できていません [37]。

我々は、Wall Street Journal (WSJ) のPenn Treebank [25] 部分を用いて約40,000文でトレーニングを行いました。また、大規模な高信頼度のBerkleyParserコーパスを使用して、約1,700万文で半教師あり学習も行いました。我々は、WSJ専用設定では16,000トークン、半教師あり設定では32,000トークンの語彙を使用しました。

全てのハイパーパラメータは英語-ドイツ語のベースモデルと同様で、Dropout、学習率、ビームサイズのみを開発セットで調整しました。推論時には、最大出力長を入力長に300を加えた値に設定し、ビームサイズ21、ペナルティ $\alpha = 0.3$ を使用しました。

表4は、WSJのセクション23における解析結果を示しており、タスク固有の調整がほとんど行われていないにもかかわらず、我々のモデルが驚くほど良好な結果を示していることを確認しました。モデルは、RNN系列-系列モデルよりも優れた結果を得ており、特にWSJの小規模データセットでのパフォーマンスが顕著です。

## 7. 結論 (Conclusion)

本論文では、トランスフォーマーを提示しました。これは、従来のエンコーダ-デコーダアーキテクチャでよく使用される再帰層をすべて排除し、自己アテンションに基づいて系列変換を行う最初のモデルです。

翻訳タスクにおいて、トランスフォーマーは再帰層や畳み込み層を使用したアーキテクチャよりもはるかに高速にトレーニングできます。WMT 2014の英語-ドイツ語および英語-フランス語の翻訳タスクにおいて、トランスフォーマーは新しい最先端のBLEUスコアを達成し、従来の最良モデルを大きく上回りました。

我々は、アテンションベースのモデルの未来に期待しており、今後も他のタスクにも適用していく予定です。特に、テキスト以外の入力および出力モダリティ（画像、音声、ビデオなど）にもトランスフォーマーを拡張し、大規模入力と出力を効率的に処理するための局所的な制限付きアテンションメカニズムを調査していく予定です。また、生成過程をより並列化することも今後の研究目標です。


# Transformer Model Relationship Diagram

1. Inputs (トークンが入力される)  
   |  
   v  
2. Input Embedding (埋め込み層でベクトルに変換)  
   |  
   v  
3. Positional Encoding (位置エンコーディングが追加される)  
   |  
   v  
4. Encoder (N層繰り返し)  
   |      
   |--- Multi-Head Attention  
   |　　　　|  
   |　　　　|--- Scaled Dot-Product Attention  
   |　　　　　　　　|  
   |　　　　　　　　|--- (クエリ、キー、バリューのテンソルによる計算)  
   |  
   |--- Feed Forward Network (フィードフォワードネットワーク)  
   |  
   v  
5. Encoder Output (エンコーダの出力が生成される)  

6. Decoder (N層繰り返し)  
   |  
   |--- Masked Multi-Head Attention  
   |　　　　|  
   |　　　　|--- Scaled Dot-Product Attention (未来の情報を隠すマスクがかけられる)  
   |　　　　　　　　|  
   |　　　　　　　　|--- (クエリ、キー、バリューのテンソルによる計算)  
   |  
   |--- Multi-Head Attention (エンコーダからの情報に基づくアテンション)  
   |　　　　|  
   |　　　　|--- Scaled Dot-Product Attention  
   |　　　　　　　　|  
   |　　　　　　　　|--- (エンコーダの出力をキー・バリューとして使用)  
   |  
   |--- Feed Forward Network (フィードフォワードネットワーク)  
   |  
   v  
7. Decoder Output (デコーダの出力が生成される)  
   |  
   v  
8. Linear & Softmax (線形変換とソフトマックスで次の単語の予測)  
   |  
   v  
9. Output (次の単語の確率分布が生成される)  
